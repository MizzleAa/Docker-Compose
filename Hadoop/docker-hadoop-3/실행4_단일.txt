##https://pearlluck.tistory.com/686 : 참고

docker pull ubuntu:20.04
docker run -it --name hadoop_sample ubuntu:20.04

apt-get update

apt-get install net-tools
apt-get install vim
apt-get install iputils-ping
apt-get install wget
apt-get install curl
apt-get install ssh

ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

wget https://corretto.aws/downloads/latest/amazon-corretto-11-x64-linux-jdk.tar.gz
tar xvfz amazon-corretto-11-x64-linux-jdk.tar.gz

mv amazon-corretto-11.0.15.9.1-linux-x64/ java
cp -r java /root/java

vim ~/.bashrc

export JAVA_HOME=/root/java
export PATH=$PATH:$JAVA_HOME/bin

source ~/.bashrc

#java 확인
java -version
javac -version

wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz
tar xvzf hadoop-3.2.3.tar.gz
mv hadoop-3.2.3 hadoop

cp -r hadoop /root/hadoop

vim ~/.bashrc

export HADOOP_HOME=/root/hadoop
export HADOOP_CONF_DIR=/root/hadoop/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root

source ~/.bashrc

vim /root/hadoop/etc/hadoop/core-site.xml

##
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
	</property>
</configuration>

vim /root/hadoop/etc/hadoop/hdfs-site.xml

##
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
</configuration>

vim /root/hadoop/etc/hadoop/mapred-site.xml
##
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
</configuration>


## 추가 환경설정 
cd hadoop/etc/hadoop/
vim hadoop-env.sh

export JAVA_HOME=/root/java
export HADOOP_HOME=/root/hadoop
export HADOOP_CONF_DIR=/root/hadoop/etc/hadoop

##port 22 connection 문제

vim etc/hosts.allow
sshd: 0.0.0.0

etc/init.d/ssh restart

* 유저 로그인 하고 진행하자

## 하둡 동작 확인 및 예제

hdfs namenode -format
hdfs datanode -format

start-dfs.sh
start-yarn.sh
start-all.sh

jps

hdfs dfsadmin -report

##########################
#SPARK

wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-without-hadoop.tgz
tar xvzf spark-3.1.2-bin-without-hadoop.tgz
mv spark-3.1.2-bin-without-hadoop spark

#python
apt-get install python3-pip -y
pip install numpy

#
vim ~/.bashrc

export SPARK_HOME=/root/spark
export SPARK_DIST_CLASSPATH=/root/hadoop/bin/hadoop
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin

source ~/.bashrc

cd $SPARK_HOME/conf

ls

cp spark-env.sh.template spark-env.sh
vim spark-env.sh

export JAVA_HOME=/root/java
export HADOOP_CONF_DIR=/root/hadoop/etc/hadoop
export YARN_CONF_DIR=/root/hadoop/etc/hadoop
export SPARK_DIST_CLASSPATH=/root/hadoop/bin/hadoop

cp workers.template workers
cp spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf

####################
docker ps
docker commit hadoop_sample hadoop:20220705

docker images
docker run -it -p 9870:9870 -p 9000:9000 -p 4040:4040 -p 8088:8088 --name hadoop_final hadoop:20220705

####################하둡 실행
service ssh start

start-all.sh